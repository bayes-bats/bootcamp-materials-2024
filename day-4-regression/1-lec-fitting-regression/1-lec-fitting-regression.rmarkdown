---
title: "Fitting regression models"
subtitle: "Day 4"
execute:
  echo: true
format: 
  revealjs:
    slide-number: true
    incremental: true
    theme: ["../../templates/slides-style.scss"]
    logo: https://www.stat.uci.edu/bayes-bats/img/logo.png
    title-slide-attributes: 
      data-background-image: https://www.stat.uci.edu/bayes-bats/img/logo.png
      data-background-size: 12%
      data-background-position: 50% 85%
---


Note that examples in this lecture are a simplified version of [Chapter 9 of Bayes Rules! book](https://www.bayesrulesbook.com/chapter-9.html).



## Packages


```{r}
library(bayesrules)
library(tidyverse)
library(rstanarm)
library(bayesplot)
library(broom.mixed)
theme_set(theme_gray(base_size = 18)) # change default font size in ggplot
```


## Data


```{r}
glimpse(bikes)
```



## Rides

:::: {.columns}

:::{.column width="60%"}


```{r}
#| echo: false
#| fig-height: 5
ggplot(data.frame(x = c(-4,4)), aes(x=x)) + 
  stat_function(fun = dnorm) + 
  labs(y = expression(paste("f(y|",mu,",",sigma,")")), x = "y (rides)") + 
  scale_x_continuous(breaks = c(-3,0,3), labels = c(expression(paste(mu,"- 3 / ",sigma)), expression(mu), expression(paste(mu,"+ 3 / ",sigma))))
```


:::

:::{.column width="40%"}

$Y_i | \mu, \sigma  \stackrel{ind}{\sim} N(\mu, \sigma^2)$  
$\mu \sim N(\theta, \tau^2)$
$\sigma  \sim \text{ some prior model.}$

:::

::::

## Regression Model

$Y_i$ the number of rides  
$X_i$ temperature (in Fahrenheit) on day $i$. 

. . .

$\mu_i = \beta_0 + \beta_1X_i$

. . .

$\beta_0:$ the typical ridership on days in which the temperature was 0 degrees ( $X_i$=0). It is not interpretable in this case.

. . . 

$\beta_1:$ the typical change in ridership for every one unit increase in temperature.



## Normal likelihood model

\begin{split}
Y_i | \beta_0, \beta_1, \sigma & \stackrel{ind}{\sim} N\left(\mu_i, \sigma^2\right) \;\; \text{ with } \;\; \mu_i = \beta_0 + \beta_1X_i \; .\\
\end{split}


```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 4
#| message: false
set.seed(454)
x <- rnorm(100, mean = 68, sd = 12)
y_1 <- -2511 + 88*x + rnorm(100, mean=0, sd = 2000)
y_2 <- -2511 + 88*x + rnorm(100, mean=0, sd = 200)
bikes_sim <- data.frame(x, y_1, y_2) %>% filter(y_1 > 0)
g1 <- ggplot(bikes_sim, aes(x=x,y=y_1)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE) + 
    #scale_x_continuous(breaks = c(25)) + 
    #scale_y_continuous(breaks = c(0,30), limits = c(min(y_1,y_2),max(y_1,y_2))) +
    lims(y = c(min(y_1,y_2),max(y_1,y_2))) + 
    labs(x = "x (temp)", y = "y (rides)")
g2 <- ggplot(bikes_sim, aes(x=x,y=y_2)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE) + 
    #scale_x_continuous(breaks = c(25)) + 
    #scale_y_continuous(breaks = c(30), limits = c(min(y_1,y_2),max(y_1,y_2))) + 
    lims(y = c(min(y_1,y_2),max(y_1,y_2))) + 
    labs(x = "x (temp)", y = "y (rides)")
    
gridExtra::grid.arrange(g1,g2,ncol=2)
  
```


##

From previous slide

Both the model lines show cases where $\beta_0 = -2000$ and slope $\beta_1 = 100$.
On the left $\sigma = 2000$ and on the right $\sigma = 200$ (right). In both cases, the model line is defined by $\beta_0 + \beta_1 x = -2000 + 100 x$.

## Model

$\text{likelihood:} \; \; \; Y_i | \beta_0, \beta_1, \sigma \;\;\;\stackrel{ind}{\sim} N\left(\mu_i, \sigma^2\right)\text{ with } \mu_i = \beta_0 + \beta_1X_i$

$\text{prior models:}$ 

$\beta_0\sim N(m_0, s_0^2 )$  
$\beta_1\sim N(m_1, s_1^2 )$  
$\sigma \sim \text{Exp}(l)$

. . .

Note: 

$\text{Exp}(l) = \text{Gamma}(1, l)$


## Model building: One step at a time 

Let $Y$ be a response variable and $X$ be a predictor or set of predictors. Then we can build a model of $Y$ by $X$ through the following general principles:

- Take note of whether $Y$ is discrete or continuous. Accordingly, identify an appropriate model structure of data $Y$ (e.g., Normal, Poisson, Binomial).
- Rewrite the mean of $Y$ as a function of predictors $X$ (e.g., $\mu = \beta_0 + \beta_1 X$).
- Identify all unknown model parameters in your model (e.g., $\beta_0, \beta_1, \sigma$).
- Take note of the values that each of these parameters might take. Accordingly, identify appropriate prior models for these parameters.

##

Suppose we have the following prior understanding of this relationship:

1. On an _average_ temperature day, say 65 or 70 degrees for D.C., there are typically around 5000 riders, though this average could be somewhere between 3000 and 7000.

2. For every one degree increase in temperature, ridership typically increases by 100 rides, though this average increase could be as low as 20 or as high as 180.

3. At any given temperature, daily ridership will tend to vary with a moderate standard deviation of 1250 rides.

##


```{r fig.height=5}
plot_normal(mean = 5000, sd = 1000) + 
  labs(x = "beta_0c", y = "pdf")
```


##


```{r fig.height=5}
plot_normal(mean = 100, sd = 40) + 
  labs(x = "beta_1", y = "pdf")
```


##



```{r fig.height=5}
plot_gamma(shape = 1, rate = 0.0008) + 
  labs(x = "sigma", y = "pdf")
```


##

$$\begin{split}
Y_i | \beta_0, \beta_1, \sigma & \stackrel{ind}{\sim} N\left(\mu_i, \sigma^2\right) \;\; \text{ with } \;\; \mu_i = \beta_0 + \beta_1X_i \\
\beta_{0c}  & \sim N\left(5000, 1000^2 \right)  \\
\beta_1  & \sim N\left(100, 40^2 \right) \\
\sigma   & \sim \text{Exp}(0.0008)  .\\
\end{split}$$

##


```{r echo = FALSE, warning=FALSE, fig.height=6, fig.width=15}
g1 <- plot_normal(mean = 5000, sd = 1000) + 
  labs(x = "beta_0c", y = "pdf")
g2 <- plot_normal(mean = 100, sd = 40) + 
  labs(x = "beta_1", y = "pdf")
g3 <- plot_gamma(shape = 1, rate = 0.0008) + 
  labs(x = "sigma", y = "pdf") + 
  lims(x = c(0,7500))
gridExtra::grid.arrange(g1,g2,g3,ncol=3)
```



## Simulation via `rstanarm`


```{r}
#|cache: true
bike_model <- stan_glm(rides ~ temp_feel, data = bikes,
                       family = gaussian,
                       prior_intercept = normal(5000, 1000),
                       prior = normal(100, 40), 
                       prior_aux = exponential(0.0008),
                       chains = 4, iter = 5000*2, seed = 84735,
                       refresh = FALSE) 
```


The `refresh = FALSE` prevents printing out your chains and iterations, especially useful in Quarto.

##


```{r}
# Effective sample size ratio and Rhat
neff_ratio(bike_model)

rhat(bike_model)

```


The effective sample size ratios are slightly above 1 and the R-hat values are very close to 1, indicating that the chains are stable, mixing quickly, and behaving much like an independent sample.

##


```{r fig.width=12}
mcmc_trace(bike_model, size = 0.1)
```


##



```{r fig.width=12, fig.height=6}
mcmc_dens_overlay(bike_model)
```


## Simulation via `rstan`


```{r}
#| eval: false
# STEP 1: DEFINE the model
stan_bike_model <- "
  data {
    int<lower = 0> n;
    vector[n] Y;
    vector[n] X;
  }
  parameters {
    real beta0;
    real beta1;
    real<lower = 0> sigma;
  }
  model {
    Y ~ normal(beta0 + beta1 * X, sigma);
    beta0 ~ normal(-2000, 1000);
    beta1 ~ normal(100, 40);
    sigma ~ exponential(0.0008);
  }
"
```


## Simulation via `rstan`


```{r}
#| eval: false
# STEP 2: SIMULATE the posterior
stan_bike_sim <- 
  stan(model_code = stan_bike_model, 
       data = list(n = nrow(bikes), Y = bikes$rides, X = bikes$temp_feel), 
       chains = 4, iter = 5000*2, seed = 84735)
```





## Posterior summary statistics


```{r echo = FALSE}
model_summary <- tidy(bike_model, effects = c("fixed", "aux"),
                      conf.int = TRUE, conf.level = 0.80)
b0_median <- model_summary[1,2]
b1_median <- model_summary[2,2]
b1_lower <- model_summary[2,4]
b1_upper <- model_summary[2,5]
```

```{r}
tidy(bike_model, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.80)
```


The __posterior median relationship__\index{posterior median relationship} is

\begin{equation}
`r round(b0_median,2)` + `r round(b1_median,2)` X.
\end{equation}

##


```{r}
# Store the 4 chains for each parameter in 1 data frame
bike_model_df <- as.data.frame(bike_model)

# Check it out
nrow(bike_model_df)
head(bike_model_df, 3)
```


##


```{r}
# 50 simulated model lines
bikes %>%
  tidybayes::add_fitted_draws(bike_model, n = 50) %>%
  ggplot(aes(x = temp_feel, y = rides)) +
    geom_line(aes(y = .value, group = .draw), alpha = 0.15) + 
    geom_point(data = bikes, size = 0.05)
```


##

Do we have ample posterior evidence that there's a positive association between ridership and temperature, i.e., that $\beta_1 > 0$? 

__Visual evidence__ In our visual examination of 50 posterior plausible scenarios for the relationship between ridership and temperature, _all_ exhibited positive associations.

__Numerical evidence from the posterior credible interval__ More rigorously, the 80% credible interval for $\beta_1$ in  `tidy()` summary, (`r round(b1_lower, 1)`, `r round(b1_upper, 1)`), lies entirely and well above 0.

__Numerical evidence from a posterior probability__ A quick tabulation approximates that there's _almost certainly_ a positive association, $P(\beta_1 > 0 \; | \; \vec{y}) \approx 1$. 


```{r}
# Tabulate the beta_1 values that exceed 0
bike_model_df %>% 
  mutate(exceeds_0 = temp_feel > 0) %>% 
  janitor::tabyl(exceeds_0)
```

## Posterior prediction

Suppose a weather report indicates that tomorrow will be a 75-degree day in D.C. What's your posterior guess of the number of riders that Capital Bikeshare should anticipate?



```{r echo = FALSE}
pred <- round(b0_median,2) + (round(b1_median,2)*75)
```


Do you think there will be `r round(pred)` riders tomorrow?

$$`r round(b0_median,2)` + `r round(b1_median,2)`*75 = `r pred` .$$

## 

There are two potential sources of variability:

- __Sampling variability__ in the data    
    The observed ridership outcomes, $Y$, typically _deviate_ from the model line. That is, we don't expect every 75-degree day to have the same exact number of rides.
    
- __Posterior variability__ in parameters $(\beta_0, \beta_1, \sigma)$    
    The posterior median model is merely the center in a _range_ of plausible model lines $\beta_0 + \beta_1 X$. We should consider this entire range as well as that in $\sigma$, the degree to which observations might deviate from the model lines.

## Posterior Predictive Model

Mathematically speaking:

$$f\left(y_{\text{new}} | \vec{y}\right) = \int\int\int f\left(y_{new} | \beta_0,\beta_1,\sigma\right) f(\beta_0,\beta_1,\sigma|\vec{y}) d\beta_0 d\beta_1 d\sigma .$$

Now, we don't actually have a nice, tidy formula for the posterior pdf of our regression parameters, $f(\beta_0,\beta_1,\sigma|\vec{y})$, and thus can't get a nice tidy formula for the posterior predictive pdf $f\left(y_{\text{new}} | \vec{y}\right)$.
What we _do_ have is 20,000 sets of parameters in the Markov chain $\left(\beta_0^{(i)},\beta_1^{(i)},\sigma^{(i)}\right)$.
We can then _approximate_ the posterior predictive model for $Y_{\text{new}}$ at $X = 75$ by simulating a ridership prediction from the Normal model evaluated each parameter set:

$$Y_{\text{new}}^{(i)} | \beta_0, \beta_1, \sigma  \; \sim \; N\left(\mu^{(i)}, \left(\sigma^{(i)}\right)^2\right) \;\; \text{ with } \;\; \mu^{(i)} = \beta_0^{(i)} + \beta_1^{(i)} \cdot 75.$$

##

Thus, each of the 20,000 parameter sets in our Markov chain (left) produces a unique prediction (right):

$$\left[
\begin{array}{lll} 
\beta_0^{(1)} & \beta_1^{(1)} & \sigma^{(1)} \\
\beta_0^{(2)} & \beta_1^{(2)} & \sigma^{(2)} \\
\vdots & \vdots & \vdots \\
\beta_0^{(20000)} & \beta_1^{(20000)} & \sigma^{(20000)} \\
\end{array}
\right]
\;\; \longrightarrow \;\;
\left[
\begin{array}{l} 
Y_{\text{new}}^{(1)} \\
Y_{\text{new}}^{(2)} \\
\vdots \\
Y_{\text{new}}^{(20000)} \\
\end{array}
\right]$$

The resulting collection of 20,000 predictions, $\left\lbrace Y_{\text{new}}^{(1)}, Y_{\text{new}}^{(2)}, \ldots, Y_{\text{new}}^{(20000)} \right\rbrace$, _approximates_ the posterior predictive model of ridership $Y$ on 75-degree days.\index{posterior predictive model}
We will obtain this approximation both "by hand," which helps us build some powerful intuition, and using shortcut R functions.


## Building a posterior predictive model

We'll simulate 20,000 predictions of ridership on a 75-degree day, $\left\lbrace Y_{\text{new}}^{(1)}, Y_{\text{new}}^{(2)}, \ldots, Y_{\text{new}}^{(20000)} \right\rbrace$, one from each parameter set in `bike_model_df`.
Let's start small with just the first posterior plausible parameter set:


```{r}
first_set <- head(bike_model_df, 1)
first_set
```

```{r echo = FALSE}
b01  <- round(first_set$`(Intercept)`)
b11  <- round(first_set$temp_feel,2)
sig1 <- round(first_set$sigma)
pred <- b01 + b11 * 75
```


Under this particular scenario, $\left(\beta_0^{(1)}, \beta_1^{(1)}, \sigma^{(1)}\right) = (`r b01`, `r b11`, `r sig1`)$, the average ridership at a given temperature is defined by

$$\mu = \beta_0^{(1)} + \beta_1^{(1)} X = `r b01` + `r b11`X  .$$

##

As such, we'd expect an __average__ of $\mu = `r round(pred)`$ riders on a 75-degree day:


```{r}
mu <- first_set$`(Intercept)` + first_set$temp_feel * 75
mu
```


##

To capture the __sampling variability__ around this average, i.e., the fact that not all 75-degree days have the same ridership, we can simulate our first official prediction $Y_{\text{new}}^{(1)}$ by taking a random draw from the Normal model specified by this first parameter set:

$$Y_{\text{new}}^{(1)} | \beta_0, \beta_1, \sigma  \; \sim \; N\left(`r pred`, `r sig1`^2\right)  .$$


```{r echo = FALSE}
set.seed(84735)
y_new <- rnorm(1, mean = mu, sd = first_set$sigma)
```


. . .

Taking a draw from this model using `rnorm()`, we happen to observe an above average `r round(y_new)` rides on the 75-degree day:


```{r}
set.seed(84735)
y_new <- rnorm(1, mean = mu, sd = first_set$sigma)
y_new
```


##

Now let's do this 19,999 more times.



```{r}
# Predict rides for each parameter set in the chain
set.seed(84735)
predict_75 <- bike_model_df %>% 
  mutate(mu = `(Intercept)` + temp_feel*75,
         y_new = rnorm(20000, mean = mu, sd = sigma))
```

```{r}
head(predict_75, 3)
```

```{r echo = FALSE}
ci_mean <- predict_75 %>% 
  summarize(post_mean = mean(mu), 
            lower_95 = quantile(mu, 0.025),
            upper_95 = quantile(mu, 0.975))

ci_new <- predict_75 %>% 
  summarize(post_mean = mean(y_new), 
            lower_95 = quantile(y_new, 0.025),
            upper_95 = quantile(y_new, 0.975))
```


Whereas the collection of 20,000 `mu` values approximates the posterior model for the _typical_ ridership on 75-degree days, $\mu = \beta_0 + \beta_1 * 75$, the 20,000 `y_new` values approximate the __posterior predictive model__ of ridership for tomorrow, an _individual_ 75-degree day, \index{posterior predictive model}

$$Y_{\text{new}} | \beta_0, \beta_1, \sigma  \; \sim \; N\left(\mu, \sigma^2\right) \;\; \text{ with } \;\; \mu = \beta_0 + \beta_1 \cdot 75 .$$

##

The 95% credible interval for the __typical__ number of rides on a 75-degree day, $\mu$, ranges from `r round(ci_mean$lower_95)` to `r round(ci_mean$upper_95)`.
In contrast, the 95% __posterior prediction interval__ for the number of rides _tomorrow_ has a much _wider_ range from `r round(ci_new$lower_95)` to `r round(ci_new$upper_95)`.



```{r}
# Construct 80% posterior credible intervals
predict_75 %>% 
  summarize(lower_mu = quantile(mu, 0.025),
            upper_mu = quantile(mu, 0.975),
            lower_new = quantile(y_new, 0.025),
            upper_new = quantile(y_new, 0.975))
```



##


```{r eval = FALSE}
# Plot the posterior model of the typical ridership on 75 degree days
ggplot(predict_75, aes(x = mu)) + 
  geom_density()

# Plot the posterior predictive model of tomorrow's ridership
ggplot(predict_75, aes(x = y_new)) + 
  geom_density()
```


##

The posterior model of $\mu$, the typical ridership on a 75-degree day (left), and the posterior predictive model of the ridership tomorrow, a specific 75-degree day (right).


```{r ch9-post-pred, fig.width = 10, echo = FALSE, fig.alt = "There are two density plots of mu, both bell-shaped and centered at mu equals 3955. However, the left density plot is much narrower, ranging from roughly 3900 to 4100. The right density plot is wider, ranging from roughly 1500 to 6500."}
g1 <- ggplot(predict_75, aes(x = mu)) + 
  geom_density() +
  lims(x = range(predict_75$y_new), y = c(0, 0.0065))
g2 <- ggplot(predict_75, aes(x = y_new)) + 
  geom_density() +
  lims(x = range(predict_75$y_new), y = c(0, 0.0065))
gridExtra::grid.arrange(g1,g2,ncol=2)
```




## 

There's more accuracy in anticipating the _average_ behavior across multiple data points than the _unique_ behavior of a single data point.



```{r fig.cap='95% posterior credible intervals (blue) for the __average__ ridership on 75-degree days (left) and the __predicted__ ridership for tomorrow, an individual 75-degree day (right).', fig.width = 4, echo = FALSE, fig.alt = "There are two scatterplots of rides (y-axis) vs temperature (x-axis). Both display the original 500 data points. The left scatterplot is superimposed with a very short vertical line at a temp_feel of 75 -- it ranges from roughly 3800 to 4100 rides. The right scatterplot is superimposed with a much wider vertical line at a temp_feel of 75 -- it ranges from roughly 1500 to 6500 rides."}
g1 <- ggplot(bikes, aes(x = temp_feel, y = rides)) + 
  geom_point(size = 0.05, color = "gray") + 
  geom_smooth(method = "lm", se = FALSE, color = "gray", size = 0.3) + 
  geom_segment(color = "blue", aes(x = 75, xend = 75, y = ci_mean$lower_95, yend = ci_mean$upper_95))
  
g2 <- ggplot(bikes, aes(x = temp_feel, y = rides)) + 
  geom_point(size = 0.05, color = "gray") + 
  geom_smooth(method = "lm", se = FALSE, color = "gray", size = 0.3) + 
  geom_segment(color = "blue", aes(x = 75, xend = 75, y = ci_new$lower_95, yend = ci_new$upper_95))
gridExtra::grid.arrange(g1,g2,ncol=2)
```



### Posterior prediction with rstanarm

Simulating the posterior predictive model from scratch allowed you to really connect with the concept, but moving forward we can utilize the `posterior_predict()` function in the __rstanarm__ package: \indexfun{posterior_predict()}


```{r}
# Simulate a set of predictions
set.seed(84735)
shortcut_prediction <- 
  posterior_predict(bike_model, newdata = data.frame(temp_feel = 75))
```


The `shortcut_prediction` object contains 20,000 predictions of ridership on 75-degree days.
We can both visualize and summarize the corresponding (approximate) posterior predictive model using our usual tricks.
The results are equivalent to those we constructed from scratch above:



```{r fig.cap = "Posterior predictive model of ridership on a 75-degree day.", fig.alt = "A density plot of the predicted ridership on a 75-degree day. The density plot is bell-shaped, centered at roughly 4000 rides, and ranges from roughly 1000 to 7000.", out.width=10}
# Construct a 95% posterior credible interval
posterior_interval(shortcut_prediction, prob = 0.95)

# Plot the approximate predictive model
mcmc_dens(shortcut_prediction) + 
  xlab("predicted ridership on a 75 degree day")
```

